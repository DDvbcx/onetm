{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import datetime\n",
    "from rdkit.Chem import rdMolDescriptors\n",
    "from rdkit import RDLogger,Chem\n",
    "from rdkit.Chem import Descriptors\n",
    "RDLogger.DisableLog('rdApp.*')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_relative_path = '../data/round1_train_data.csv'\n",
    "test_relative_path = '../data/round1_test_data.csv'\n",
    "\n",
    "train_df = pd.read_csv(train_relative_path)\n",
    "test_df = pd.read_csv(test_relative_path)\n",
    "\n",
    "print(f'Training set size: {len(train_df)}, test set size: {len(test_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将分子转化为定长的指纹向量\n",
    "def mfgen(mol,nBits=2048, radius=2):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    mol : mol\n",
    "        RDKit mol object.\n",
    "    nBits : int\n",
    "        Number of bits for the fingerprint.\n",
    "    radius : int\n",
    "        Radius of the Morgan fingerprint.\n",
    "    Returns\n",
    "    -------\n",
    "    mf_desc_map : ndarray\n",
    "        ndarray of molecular fingerprint descriptors.\n",
    "    '''\n",
    "    # 返回分子的位向量形式的Morgan fingerprint\n",
    "    fp = rdMolDescriptors.GetMorganFingerprintAsBitVect(mol,radius=radius,nBits=nBits)\n",
    "    return np.array(list(map(eval,list(fp.ToBitString()))))\n",
    "\n",
    "# 加载数据\n",
    "def vec_cpd_lst(smi_lst):\n",
    "    smi_set = list(set(smi_lst))\n",
    "    smi_vec_map = {}\n",
    "    for smi in tqdm(smi_set): # tqdm：显示进度条\n",
    "        mol = Chem.MolFromSmiles(smi)\n",
    "        smi_vec_map[smi] = mfgen(mol)\n",
    "    smi_vec_map[''] = np.zeros(2048)\n",
    "    \n",
    "    vec_lst = [smi_vec_map[smi] for smi in smi_lst]\n",
    "    return np.array(vec_lst)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取 SMILES 字符串, 计算每个 SMILES 的 logP 值和 TPSA 值\n",
    "def get_logP_and_TPSA(df, cols):\n",
    "    # 检查输入列是否存在\n",
    "    for col in cols:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"Column '{col}' not found in DataFrame\")\n",
    "\n",
    "    for col in cols:\n",
    "        # 创建存储logP值和TPSA值的列表\n",
    "        logP_values = []\n",
    "        tpsa_values = []\n",
    "        \n",
    "        # 遍历每一行的SMILES字符串并计算LogP和TPSA值\n",
    "        for smile in df[col]:\n",
    "            try:\n",
    "                # 从SMILES字符串创建分子对象\n",
    "                mol = Chem.MolFromSmiles(smile)\n",
    "                if mol is not None:\n",
    "                    # 计算LogP值\n",
    "                    logP = Descriptors.MolLogP(mol)\n",
    "                    logP_values.append(logP)\n",
    "                    \n",
    "                    # 计算TPSA值\n",
    "                    tpsa = Descriptors.TPSA(mol)\n",
    "                    tpsa_values.append(tpsa)\n",
    "                else:\n",
    "                    logP_values.append(None)  # 无效的SMILES\n",
    "                    tpsa_values.append(None)  # 无效的SMILES\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing SMILES {smile}: {e}\")\n",
    "                logP_values.append(None)  # 处理错误情况\n",
    "                tpsa_values.append(None)  # 处理错误情况\n",
    "\n",
    "        # 将计算的LogP值和TPSA值添加到数据框中\n",
    "        df[f'logP_{col}'] = logP_values\n",
    "        df[f'TPSA_{col}'] = tpsa_values\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['Reactant1', 'Reactant2', 'Product', 'Additive', 'Solvent']\n",
    "train_df1 = get_logP_and_TPSA(train_df, cols)\n",
    "test_df1 = get_logP_and_TPSA(test_df, cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从csv中读取数据\n",
    "train_rct1_smi = train_df['Reactant1'].to_list()\n",
    "train_rct2_smi = train_df['Reactant2'].to_list()\n",
    "train_add_smi = train_df['Additive'].to_list()\n",
    "train_sol_smi = train_df['Solvent'].to_list()\n",
    "\n",
    "# 将SMILES转化为分子指纹\n",
    "train_rct1_fp = vec_cpd_lst(train_rct1_smi)\n",
    "train_rct2_fp = vec_cpd_lst(train_rct2_smi)\n",
    "train_add_fp = vec_cpd_lst(train_add_smi)\n",
    "train_sol_fp = vec_cpd_lst(train_sol_smi)\n",
    "# 在dim=1维度进行拼接。即：将一条数据的Reactant1,Reactant2,Product,Additive,Solvent字段的morgan fingerprint拼接为一个向量。\n",
    "train_x = np.concatenate([train_rct1_fp,train_rct2_fp,train_add_fp,train_sol_fp],axis=1)\n",
    "train_y = train_df['Yield'].to_numpy()\n",
    "\n",
    "# 测试集也进行同样的操作\n",
    "test_rct1_smi = test_df['Reactant1'].to_list()\n",
    "test_rct2_smi = test_df['Reactant2'].to_list()\n",
    "test_add_smi = test_df['Additive'].to_list()\n",
    "test_sol_smi = test_df['Solvent'].to_list()\n",
    "\n",
    "test_rct1_fp = vec_cpd_lst(test_rct1_smi)\n",
    "test_rct2_fp = vec_cpd_lst(test_rct2_smi)\n",
    "test_add_fp = vec_cpd_lst(test_add_smi)\n",
    "test_sol_fp = vec_cpd_lst(test_sol_smi)\n",
    "test_x = np.concatenate([test_rct1_fp,test_rct2_fp,test_add_fp,test_sol_fp],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将读取到的分子向量特征与分子性质合并\n",
    "\n",
    "X_train_1 = pd.DataFrame(train_x)\n",
    "X_test_1 = pd.DataFrame(test_x)\n",
    "\n",
    "cols = ['logP_Reactant1', 'TPSA_Reactant1', 'logP_Reactant2', 'TPSA_Reactant2', 'logP_Product', 'TPSA_Product', 'logP_Additive',\n",
    "       'TPSA_Additive', 'logP_Solvent', 'TPSA_Solvent']\n",
    "for col in cols:\n",
    "    X_train_1 = pd.concat([X_train_1, train_df1[col]], axis=1)\n",
    "    X_test_1 = pd.concat([X_test_1, test_df1[col]], axis=1)\n",
    "    \n",
    "# 将所有列名转换为字符串\n",
    "X_train_1.columns = X_train_1.columns.astype(str)\n",
    "X_test_1.columns = X_test_1.columns.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特征工程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 删除全为零的特征列\n",
    "def eliminate_zeros(df1, df2, threshold=1):\n",
    "    ''' 删除全为零的特征列 '''\n",
    "    \n",
    "    df_1_zero = (df1 == 0)\n",
    "    zero_count1 = df_1_zero.sum(axis=0)\n",
    "    zeros1_ratio = zero_count1 / len(df1)\n",
    "\n",
    "    # Identify columns with zero ratio equal to threshold\n",
    "    columns_to_remove = zeros1_ratio[zeros1_ratio >= threshold].index\n",
    "    \n",
    "    # Drop the columns from both DataFrames\n",
    "    df_1 = df1.drop(columns=columns_to_remove, axis=1)\n",
    "    df_2 = df2.drop(columns=columns_to_remove, axis=1)\n",
    "\n",
    "    return df_1, df_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_temp, X_test_temp = eliminate_zeros(X_train_1, X_test_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用模型进行特征选择\n",
    "__说明__ : 这一步比较费时间，我在机器上跑了大概 60 多分钟。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用模型进行特征选择\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# Initialize GradientBoostingRegressor with parameters\n",
    "gbdt = GradientBoostingRegressor(n_estimators=1000, learning_rate=0.01, max_depth=13,subsample=0.7)\n",
    "\n",
    "\n",
    "gbdt.fit(X_train_temp, train_y)\n",
    "\n",
    "selector = SelectFromModel(gbdt, prefit=True, threshold=\"mean\")\n",
    "\n",
    "\n",
    "X_train2 = selector.transform(X_train_temp)\n",
    "X_test2 = selector.transform(X_test_temp)\n",
    "\n",
    "print(X_train2.shape, X_test2.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用 GBDT 模型进行预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_gbdt_and_predict(X_train, y_train, X_test):\n",
    "    ''' 使用 GBDT 训练并预测测试集 '''\n",
    "    \n",
    "    # 定义 GBDT 参数\n",
    "    gbdt_params = {\n",
    "        'learning_rate': 0.01,\n",
    "        'max_depth': 20,\n",
    "        'max_features': 'sqrt',\n",
    "        'min_samples_leaf': 2,\n",
    "        'min_samples_split': 5,\n",
    "        'n_estimators': 1000,\n",
    "        'subsample': 0.8,\n",
    "    }\n",
    "    \n",
    "    # 初始化并训练 GBDT 模型\n",
    "    gbdt = GradientBoostingRegressor(**gbdt_params)\n",
    "    gbdt.fit(X_train, y_train)\n",
    "        \n",
    "    # 预测测试集\n",
    "    y_pred_test = gbdt.predict(X_test)\n",
    "    \n",
    "    return y_pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = train_gbdt_and_predict(X_train2, train_y, X_test2)\n",
    "\n",
    "# 创建提交文件的内容\n",
    "ans_str_lst = ['rxnid,Yield']\n",
    "for idx, y in enumerate(y_pred):\n",
    "    ans_str_lst.append(f'test{idx+1},{y:.4f}')\n",
    "\n",
    "# 将内容转换为 DataFrame\n",
    "# DataFrame 需要的格式是两列，第一列是 'rxnid', 第二列是 'Yield'\n",
    "df = pd.DataFrame([x.split(',') for x in ans_str_lst[1:]], columns=['rxnid', 'Yield'])\n",
    "\n",
    "# 生成当前时间的时间戳\n",
    "timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "# 创建文件名\n",
    "filename = f\"../submit/submit_{timestamp}.txt\"\n",
    "\n",
    "# 保存 DataFrame 为 CSV 文件，不包含列头和索引\n",
    "df.to_csv(filename, header=True, index=False)\n",
    "\n",
    "print(f\"提交文件已保存为 {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_temp_3, train_y, test_size=0.2, random_state=42)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import make_scorer, r2_score\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "def train_gbdt(X_train, y_train, X_val, y_val, training_name):\n",
    "    ''' 使用 GBDT 训练'''\n",
    "    \n",
    "    gbdt_params = {\n",
    "    'learning_rate': 0.01,\n",
    "    'max_depth': 20,\n",
    "    'max_features': 'sqrt',\n",
    "    'min_samples_leaf': 2,\n",
    "    'min_samples_split': 5,\n",
    "    'n_estimators': 1700,\n",
    "\n",
    "    }\n",
    "    gbdt = GradientBoostingRegressor(**gbdt_params)\n",
    "    gbdt.fit(X_train, y_train)\n",
    "     \n",
    "    y_pred = gbdt.predict(X_val)\n",
    "    r2 = r2_score(y_val, y_pred)\n",
    "\n",
    "    print(f\" 使用 GBDT 在 {training_name} 数据集上的 R2 分数: {r2}\")\n",
    "\n",
    "train_gbdt(X_train, y_train, X_val, y_val, \"lgbm\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
